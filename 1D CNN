import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from keras import Sequential
from keras import layers, optimizers
from sklearn.model_selection import train_test_split as tts
import pandas as pd
import os
os.environ["PATH"] += os.pathsep + "C:/Program Files/Graphviz/bin"

files = [r"C:\masters\machine learning\january.csv", r"C:\masters\machine learning\Provan List\02081996.csv", r"C:\masters\machine learning\Provan List\03011996.csv",
         r"C:\masters\machine learning\Provan List\03041996.csv",r"C:\masters\machine learning\Provan List\03081996.csv",r"C:\masters\machine learning\Provan List\04041996.csv",
         r"C:\masters\machine learning\Provan List\04081996.csv", r"C:\masters\machine learning\Provan List\05041996.csv", r"C:\masters\machine learning\Provan List\06081996.csv",
         r"C:\masters\machine learning\Provan List\07041996.csv", r"C:\masters\machine learning\Provan List\08021996.csv", r"C:\masters\machine learning\Provan List\08041996.csv",
         r"C:\masters\machine learning\Provan List\11081995.csv", r"C:\masters\machine learning\Provan List\11091995.csv", r"C:\masters\machine learning\Provan List\12051996.csv", 
         r"C:\masters\machine learning\Provan List\13071995.csv", r"C:\masters\machine learning\Provan List\13101995.csv", r"C:\masters\machine learning\Provan List\14051996.csv",
         r"C:\masters\machine learning\Provan List\14071995.csv", r"C:\masters\machine learning\Provan List\14081995.csv", r"C:\masters\machine learning\Provan List\14091996.csv",
         r"C:\masters\machine learning\Provan List\14101995.csv", r"C:\masters\machine learning\Provan List\15081995.csv", r"C:\masters\machine learning\Provan List\18011996.csv", 
         r"C:\masters\machine learning\Provan List\19011996.csv", r"C:\masters\machine learning\Provan List\19031996.csv", r"C:\masters\machine learning\Provan List\24091995.csv",
         r"C:\masters\machine learning\Provan List\24091996.csv", r"C:\masters\machine learning\Provan List\25011996.csv", r"C:\masters\machine learning\Provan List\25091996.csv",
         r"C:\masters\machine learning\Provan List\26011996.csv", r"C:\masters\machine learning\Provan List\27011996.csv"]

df = pd.concat((pd.read_csv(file) for file in files), ignore_index=True)

df['time'] = pd.to_datetime(df['time'], format='%Y-%m-%d %H:%M:%S')
np.int64(df['time'])
df['time'] = df['time'].astype(np.int64)

df['p_l'] = pd.to_numeric(df['p_l'], errors='coerce')
df['v'] = pd.to_numeric(df['v'], errors='coerce')

def min_max_scaling(df, column_name, a, b):
    min_value = df[column_name].min()
    max_value = df[column_name].max()
    df[column_name] = a * (df[column_name] - min_value) / (max_value - min_value) - b

min_max_scaling(df, 'p_l', 1, 0)
min_max_scaling(df, 'v', 2, 1)
min_max_scaling(df, 'time', 1, 0)

df.dropna(inplace=True)
#=========================================== DO NOT CHANGE THIS PART ====================================================
data = df.values  
batch_size = 101

batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]  # List of arrays

np.random.shuffle(batches)

shuffled_data = np.concatenate(batches, axis=0)  # Merge batches back into a single array
df_shuffled = pd.DataFrame(shuffled_data, columns=df.columns)
print(df_shuffled)

train, test = tts(df_shuffled, test_size=0.1, shuffle=False)  

train_y = train.pop('event')
test_y = test.pop('event')

train_y = train_y.values.reshape(-1, 1)
test_y = test_y.values.reshape(-1, 1)

tf.convert_to_tensor(train.values)
tf.convert_to_tensor(test.values)

train = tf.expand_dims(train.values, axis=-1)
test = tf.expand_dims(test.values, axis=-1)

#=========================================== END OF DO NOT CHANGE ====================================================

train_ds = tf.data.Dataset.from_tensor_slices((train, train_y)).batch(16)
test_ds = tf.data.Dataset.from_tensor_slices((test, test_y)).batch(16)

#for feature, event in train_ds.take(1):
 #   print('Features: {}, Event: {}'.format(feature, event))
print("Unique labels:", np.unique(train_y))

class MyModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.conv1 = layers.Conv1D(64, 5, activation='relu', padding='same')
        self.bn1 = layers.BatchNormalization()
        self.conv2 = layers.Conv1D(128, 5, activation='relu', padding='same')
        self.bn2 = layers.BatchNormalization()
        self.conv3 = layers.Conv1D(256, 5, activation='relu', padding='same')  
        self.bn3 = layers.BatchNormalization()
        self.flatten = layers.Flatten()
        self.d1 = layers.Dense(128, activation='relu')
        self.drop1 = layers.Dropout(0.3) 
        self.d2 = layers.Dense(64, activation='relu')
        self.drop2 = layers.Dropout(0.3)  
        self.final = layers.Dense(1, activation="sigmoid")  

    def call(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.conv3(x)
        x = self.bn3(x)
        x = self.flatten(x)
        x = self.d1(x)
        x = self.drop1(x)
        x = self.d2(x)
        x = self.drop2(x)
        return self.final(x)

model = MyModel()
loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)

optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)
train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')

@tf.function
def train_step(features, labels):
    with tf.GradientTape() as tape:
        predictions = model(features, training=True)
        loss = loss_object(labels, predictions)
        
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_loss(loss)
    train_accuracy(labels, predictions)

@tf.function
def test_step(features, labels):
    predictions = model(features, training=False)
    t_loss = loss_object(labels, predictions)

    test_loss(t_loss)
    test_accuracy(labels, predictions)

for features, labels in train_ds.take(1):
    preds = model(features)
    print("Predictions:", preds.numpy().flatten()[:10])
    print("Labels:", labels.numpy().flatten()[:10])

EPOCHS = 100

for epoch in range(EPOCHS):
    accuracy_train_list = []
    test_accuracy_list = []
    loss_train_list = []
    test_loss_list = []
    # Reset the metrics at the start of the next epoch
    train_loss.reset_state()
    train_accuracy.reset_state()
    test_loss.reset_state()
    test_accuracy.reset_state()

    for features, labels in train_ds:
        train_step(features, labels)

    for test_features, test_labels in test_ds:
        test_step(test_features, test_labels)

    print(
        f'Epoch {epoch + 1}, '
        f'Loss: {train_loss.result():0.2f}, '
        f'Accuracy: {train_accuracy.result() * 100:0.2f}, '
        f'Test Loss: {test_loss.result():0.2f}, '
        f'Test Accuracy: {test_accuracy.result() * 100:0.2f}'
    )
    accuracy_train_list.append(train_accuracy.result() * 100)
    test_accuracy_list.append(test_accuracy.result() * 100)
    loss_train_list.append(train_loss.result())
    test_loss_list.append(test_loss.result())
